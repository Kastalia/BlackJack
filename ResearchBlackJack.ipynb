{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjack_original as jack\n",
    "import blackjack_double as jack_double\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наивная стратегия: делать stand при сумме карт 16-21, иначе hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - stand\n",
    "# 1 - hit\n",
    "env = jack.BlackjackEnv(natural=True)\n",
    "\n",
    "episods = 100000\n",
    "state_stand = [16, 17, 18, 19, 20, 21]\n",
    "reward_average = 0\n",
    "step_average = 0\n",
    "\n",
    "for i in tqdm(range(episods),desc=\"TRAINING\"):\n",
    "    player, dealer, usable_ace = env.reset()\n",
    "    done = False\n",
    "    step=1\n",
    "    while(done==False):\n",
    "        step+=1\n",
    "        if player in state_stand:\n",
    "            (player, dealer, usable_ace), reward, done, _ = env.step(0)            \n",
    "        else:\n",
    "            (player, dealer, usable_ace), reward, done, _ = env.step(1)\n",
    "    reward_average += reward\n",
    "    step_average += step\n",
    "    \n",
    "reward_average = reward_average/episods\n",
    "step_average = step_average/episods\n",
    "\n",
    "print(f\"Наивная стратегия делать stand при сумме {state_stand}\\nСредний доход:{reward_average}\\nСреднее кол-во ходов:{step_average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-policy TD control \"Q learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDcontrol():\n",
    "    def __init__(self, gym_env, epsilon, alpha, gamma):\n",
    "        self.env = gym_env\n",
    "        \n",
    "        A_count = self.env.action_space.n\n",
    "        S_shape = []\n",
    "        for space in self.env.observation_space:\n",
    "            S_shape.append(space.n)\n",
    "        shape = S_shape\n",
    "        shape.append(A_count)\n",
    "        self.Q = np.zeros((shape))\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma        \n",
    "            \n",
    "    def set(self,):\n",
    "        pass\n",
    "    \n",
    "    def train(self, episods):\n",
    "        for _ in tqdm(range(episods),desc=\"TRAINING\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not(done):\n",
    "                action = self.__get_action(state)\n",
    "                state_new, reward, done, _ = self.env.step(action)\n",
    "                self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[state_new]) - self.Q[state][action])\n",
    "                state = state_new\n",
    "    \n",
    "    def game(self, episods):\n",
    "        reward_average = 0\n",
    "        step_average = 0\n",
    "        for _ in tqdm(range(episods),desc=\"GAMING\"):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            step = 1\n",
    "            while not(done):\n",
    "                action = self.pi(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                step+=1\n",
    "            reward_average+=reward\n",
    "            step_average+=step\n",
    "            \n",
    "        return reward_average/episods, step_average/episods\n",
    "                \n",
    "    def pi(self, state):\n",
    "        return np.argmax(self.Q[state])\n",
    "        \n",
    "    def __get_action(self, state):\n",
    "        if np.random.uniform(0,1)<self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "        \n",
    "    def __retr(self,player, dealer, usable_ace):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack.BlackjackEnv(natural=True)\n",
    "\n",
    "alphas = [1e-6, 1e-5, 1e-4]\n",
    "gammas = [0.5, 0.8, 0.9, 1]\n",
    "epsilons = [0.8, 0.85, 0.9, 0.95]\n",
    "episods_count = 200000\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "            qmodel.train(episods_count)\n",
    "            rew,step = qmodel.game(episods_count)\n",
    "            result = f\"{rew} steps average={step} alpha={alpha} gamma={gamma} epsilon={epsilon}\"\n",
    "            results.append(result)\n",
    "            print(\"Off-policy TD control (Q learning)\")\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-0.0286625 steps average=2.517175 alpha=1e-05 gamma=0.8 epsilon=0.9\n",
    "-0.0221275 steps average=2.509645 alpha=1e-05 gamma=0.9 epsilon=0.9\n",
    "-0.0272775 steps average=2.527905 alpha=0.0001 gamma=0.8 epsilon=0.85\n",
    "-0.0278525 steps average=2.522095 alpha=0.0001 gamma=0.8 epsilon=0.9\n",
    "-0.0279025 steps average=2.51747 alpha=0.0001 gamma=0.9 epsilon=0.85\n",
    "-0.027605 steps average=2.518605 alpha=0.0001 gamma=0.9 epsilon=0.95\n",
    "-0.0275075 steps average=2.532325 alpha=0.0001 gamma=1 epsilon=0.85\n",
    "\n",
    "-0.0270925 steps average=2.495345 alpha=1e-06 gamma=1 epsilon=0.99\n",
    "-0.02758 steps average=2.521735 alpha=1e-05 gamma=0.5 epsilon=0.9\n",
    "-0.0273425 steps average=2.521985 alpha=0.0001 gamma=0.5 epsilon=0.9\n",
    "-0.02724 steps average=2.51686 alpha=0.0001 gamma=0.9 epsilon=0.999\n",
    "-0.0275925 steps average=2.5243alpha=0.0001 gamma=1 epsilon=0.9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlackJack with Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack_double.BlackjackEnv(natural=True)\n",
    "\n",
    "alphas = [1e-6, 1e-5, 1e-4]\n",
    "gammas = [0.5, 0.8, 0.9, 1]\n",
    "epsilons = [0.8, 0.85, 0.9, 0.95]\n",
    "episods_count = 200000\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "            qmodel.train(episods_count)\n",
    "            rew,step = qmodel.game(episods_count)\n",
    "            result = f\"{rew} steps average={step} alpha={alpha} gamma={gamma} epsilon={epsilon}\"\n",
    "            results.append(result)\n",
    "            print(\"Off-policy TD control (Q learning)\")\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-0.0152325 steps average=2.39135 alpha=1e-06 gamma=0.8 epsilon=0.95\n",
    "-0.0179575 steps average=2.379765 alpha=1e-06 gamma=1 epsilon=0.85\n",
    "-0.0189975 steps average=2.37266 alpha=1e-05 gamma=0.8 epsilon=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]*4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck.remove(10)\n",
    "len(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.zeros((2,3,4))\n",
    "arr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(0,10,size=2*3*4).reshape(2,3,4)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (0,int(True))\n",
    "arr[state][0]=400\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    return float(a > b) - float(a < b)\n",
    "\n",
    "cmp(12, 13)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
