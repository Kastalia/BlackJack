{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjack_original as jack\n",
    "import blackjack_double as jack_double\n",
    "import blackjack_deckofcard as jack_countdeck\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наивная стратегия: делать stand при сумме карт 16-21, иначе hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - stand\n",
    "# 1 - hit\n",
    "env = jack.BlackjackEnv(natural=True)\n",
    "\n",
    "episods = 100000\n",
    "state_stand = [16, 17, 18, 19, 20, 21]\n",
    "reward_average = 0\n",
    "step_average = 0\n",
    "\n",
    "for i in tqdm(range(episods),desc=\"TRAINING\"):\n",
    "    player, dealer, usable_ace = env.reset()\n",
    "    done = False\n",
    "    step=1\n",
    "    while(done==False):\n",
    "        step+=1\n",
    "        if player in state_stand:\n",
    "            (player, dealer, usable_ace), reward, done, _ = env.step(0)            \n",
    "        else:\n",
    "            (player, dealer, usable_ace), reward, done, _ = env.step(1)\n",
    "    reward_average += reward\n",
    "    step_average += step\n",
    "    \n",
    "reward_average = reward_average/episods\n",
    "step_average = step_average/episods\n",
    "\n",
    "print(f\"Наивная стратегия делать stand при сумме {state_stand}\\nСредний доход:{reward_average}\\nСреднее кол-во ходов:{step_average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-policy TD control \"Q learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDcontrol():\n",
    "    def __init__(self, gym_env, epsilon, alpha, gamma):\n",
    "        self.env = gym_env\n",
    "        \n",
    "        A_count = self.env.action_space.n\n",
    "        S_shape = []\n",
    "        for space in self.env.observation_space:\n",
    "            S_shape.append(space.n)\n",
    "        shape = S_shape\n",
    "        shape.append(A_count)\n",
    "        self.Q = np.zeros((shape))\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma        \n",
    "            \n",
    "    def set(self,):\n",
    "        pass\n",
    "    \n",
    "    def train(self, episods, show_plots = False):        \n",
    "        if show_plots:\n",
    "            batch_count = 10\n",
    "            game_size = 100000\n",
    "            batch_size = episods//batch_count            \n",
    "            x_episods = []\n",
    "            y_rewards = []           \n",
    "\n",
    "            for index_batch in range(batch_count):\n",
    "                self.__trainloop(batch_size, disable_tqdm=True)\n",
    "                reward_average, _ = self.game(game_size, disable_tqdm=True)\n",
    "                # графики\n",
    "                x_episods.append((index_batch+1)*batch_size)\n",
    "                y_rewards.append(reward_average)\n",
    "                clear_output(True)\n",
    "                plt.figure(figsize=(16, 5))\n",
    "                plt.xlabel('Количество эпизодов обучения')\n",
    "                plt.ylabel(f'Награда (расчитана как усреднение {game_size} партий.)')\n",
    "                plt.plot(x_episods, y_rewards, linewidth=1)\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            self.__trainloop(episods)\n",
    "\n",
    "    \n",
    "    \n",
    "    def game(self, episods, disable_tqdm=False):\n",
    "        reward_average = 0\n",
    "        step_average = 0\n",
    "        for _ in tqdm(range(episods),desc=\"GAMING\", disable=disable_tqdm):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            step = 1\n",
    "            while not(done):\n",
    "                action = self.pi(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                step+=1\n",
    "            reward_average+=reward\n",
    "            step_average+=step\n",
    "            \n",
    "        return reward_average/episods, step_average/episods\n",
    "                \n",
    "    def pi(self, state):\n",
    "        return np.argmax(self.Q[state])\n",
    "        \n",
    "    def __get_action(self, state):\n",
    "        if np.random.uniform(0,1)<self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "        \n",
    "    def __retr(self,player, dealer, usable_ace):\n",
    "        pass\n",
    "    \n",
    "    def __trainloop(self, iterations, disable_tqdm=False):\n",
    "        for _ in tqdm(range(iterations),desc=\"TRAINING\", disable=disable_tqdm):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not(done):\n",
    "                action = self.__get_action(state)\n",
    "                state_new, reward, done, _ = self.env.step(action)\n",
    "                self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + self.gamma*np.max(self.Q[state_new]) - self.Q[state][action])\n",
    "                state = state_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack.BlackjackEnv(natural=True)\n",
    "\n",
    "alphas = [1e-6, 1e-5, 1e-4]\n",
    "gammas = [0.5, 0.8, 0.9, 1]\n",
    "epsilons = [0.8, 0.85, 0.9, 0.95]\n",
    "episods_train = 100000\n",
    "episods_game = 100000\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "            qmodel.train(episods_train)\n",
    "            rew,_ = qmodel.game(episods_game)\n",
    "            result = f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\"\n",
    "            results.append(result)\n",
    "            print(\"Off-policy TD control (Q learning)\")\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-0.0286625 steps average=2.517175 alpha=1e-05 gamma=0.8 epsilon=0.9\n",
    "-0.0221275 steps average=2.509645 alpha=1e-05 gamma=0.9 epsilon=0.9\n",
    "-0.0272775 steps average=2.527905 alpha=0.0001 gamma=0.8 epsilon=0.85\n",
    "-0.0278525 steps average=2.522095 alpha=0.0001 gamma=0.8 epsilon=0.9\n",
    "-0.0279025 steps average=2.51747 alpha=0.0001 gamma=0.9 epsilon=0.85\n",
    "-0.027605 steps average=2.518605 alpha=0.0001 gamma=0.9 epsilon=0.95\n",
    "-0.0275075 steps average=2.532325 alpha=0.0001 gamma=1 epsilon=0.85\n",
    "\n",
    "-0.0270925 steps average=2.495345 alpha=1e-06 gamma=1 epsilon=0.99\n",
    "-0.02758 steps average=2.521735 alpha=1e-05 gamma=0.5 epsilon=0.9\n",
    "-0.0273425 steps average=2.521985 alpha=0.0001 gamma=0.5 epsilon=0.9\n",
    "-0.02724 steps average=2.51686 alpha=0.0001 gamma=0.9 epsilon=0.999\n",
    "-0.0275925 steps average=2.5243alpha=0.0001 gamma=1 epsilon=0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack.BlackjackEnv(natural=True)\n",
    "epsilon = 0.9\n",
    "alpha = 1e-05\n",
    "gamma = 0.9\n",
    "episods_train = 200000\n",
    "episods_game = 100000\n",
    "qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "\n",
    "qmodel.train(episods_train, show_plots = True)\n",
    "rew,_ = qmodel.game(episods_game)\n",
    "print(\"Off-policy TD control (Q learning)\")\n",
    "print(f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlackJack with Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack_double.BlackjackEnv(natural=True)\n",
    "\n",
    "alphas = [1e-6, 1e-5, 1e-4]\n",
    "gammas = [0.5, 0.8, 0.9, 1]\n",
    "epsilons = [0.8, 0.85, 0.9, 0.95]\n",
    "episods_train = 100000*2\n",
    "episods_game = 100000\n",
    "\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "            qmodel.train(episods_train)\n",
    "            rew,_ = qmodel.game(episods_game)\n",
    "            result = f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\"\n",
    "            results.append(result)\n",
    "            print(\"Off-policy TD control (Q learning)\")\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-0.0152325 steps average=2.39135 alpha=1e-06 gamma=0.8 epsilon=0.95\n",
    "-0.0179575 steps average=2.379765 alpha=1e-06 gamma=1 epsilon=0.85\n",
    "-0.0189975 steps average=2.37266 alpha=1e-05 gamma=0.8 epsilon=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack_double.BlackjackEnv(natural=True)\n",
    "epsilon = 0.9\n",
    "alpha = 1e-05\n",
    "gamma = 0.9\n",
    "episods_train = 100000*2\n",
    "episods_game = 100000\n",
    "qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "\n",
    "qmodel.train(episods_train, show_plots = True)\n",
    "rew,_ = qmodel.game(episods_game)\n",
    "print(\"Off-policy TD control (Q learning)\")\n",
    "print(f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlackJack with Double  c подсчетом карт системой «Половинки» "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack_countdeck.BlackjackEnv(natural=True)\n",
    "\n",
    "alphas = [1e-6, 1e-5, 1e-4]\n",
    "gammas = [0.5, 0.8, 0.9, 1]\n",
    "epsilons = [0.8, 0.85, 0.9, 0.95]\n",
    "episods_train = 100000*2*89\n",
    "episods_game = 100000\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        for epsilon in epsilons:\n",
    "            qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "            qmodel.train(episods_train)\n",
    "            rew,_ = qmodel.game(episods_game)\n",
    "            result = f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\"\n",
    "            results.append(result)\n",
    "            print(\"Off-policy TD control (Q learning)\")\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-0.024049 steps average=2.348123333333333 alpha=2e-05 gamma=1 epsilon=1 episods=1 500 000\n",
    "-0.011602055555555555 steps average=2.3673052222222224 alpha=2e-05 gamma=1 epsilon=1 episods=9 000 000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jack_countdeck.BlackjackEnv(natural=True)\n",
    "epsilon = 0.9\n",
    "alpha = 1e-05\n",
    "gamma = 0.9\n",
    "episods_train = 100000*2*89\n",
    "episods_game = 100000\n",
    "qmodel = TDcontrol(env, epsilon, alpha, gamma)\n",
    "\n",
    "qmodel.train(episods_train, show_plots = True)\n",
    "rew,_ = qmodel.game(episods_game)\n",
    "print(\"Off-policy TD control (Q learning)\")\n",
    "print(f\"{rew} train episods={episods_train} game episods={episods_game} alpha={alpha} gamma={gamma} epsilon={epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
